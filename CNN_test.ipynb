{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:14.566703Z",
     "start_time": "2024-12-05T12:07:14.554868Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Get unique classes actually present in the data\n",
    "    unique_classes = np.unique(all_labels)\n",
    "    target_names = [f'DoR Group {i}' for i in unique_classes]\n",
    "    \n",
    "    return classification_report(all_labels, all_preds, \n",
    "                               target_names=target_names)\n",
    "\n",
    "\n",
    "def get_sdss_image_url(plate, mjd, fiber, scale=0.4, width=512, height=512):\n",
    "    \"\"\"\n",
    "    Constructs URL for SDSS image cutout using Legacy Survey\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get RA/DEC from your local FITS files\n",
    "        fits_filename = f\"fits_shortlist/spec-{plate:04d}-{mjd}-{fiber:04d}.fits\"\n",
    "        \n",
    "        try:\n",
    "            with fits.open(fits_filename) as hdul:\n",
    "                ra = hdul[0].header['PLUG_RA']\n",
    "                dec = hdul[0].header['PLUG_DEC']\n",
    "                # print(f\"Found coordinates for {fits_filename}: RA={ra}, DEC={dec}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading FITS file {fits_filename}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        # Use Legacy Survey instead - more reliable\n",
    "        pixel_scale = 0.262  # arcsec/pixel\n",
    "        size = int(width * scale / pixel_scale)  # convert to Legacy Survey pixel size\n",
    "        \n",
    "        base_url = \"https://www.legacysurvey.org/viewer/jpeg-cutout\"\n",
    "        params = {\n",
    "            'ra': ra,\n",
    "            'dec': dec,\n",
    "            'width': size,\n",
    "            'height': size,\n",
    "            'layer': 'sdss',\n",
    "            'pixscale': pixel_scale, \n",
    "            'quality': 100,\n",
    "            'bands': 'grz'  # Better for seeing galaxy structure\n",
    "\n",
    "        }\n",
    "        \n",
    "        url = f\"{base_url}?\"\n",
    "        url += \"&\".join([f\"{key}={value}\" for key, value in params.items()])\n",
    "        \n",
    "        # print(f\"Generated URL: {url}\")\n",
    "        \n",
    "        return url, ra, dec\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {plate}-{mjd}-{fiber}: {str(e)}\")\n",
    "        return None    \n",
    "    \n",
    "def download_image(url, output_path, max_retries=3):\n",
    "    \"\"\"Downloads image from URL and saves to specified path\"\"\"\n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Extract base filename without numbers before .jpg\n",
    "    base_path = re.sub(r'\\s*\\d+\\.jpg$', '.jpg', output_path)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    img = Image.open(io.BytesIO(response.content))\n",
    "                    img = img.convert('RGB')\n",
    "                    img.save(base_path)\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving image to {base_path}: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return False\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Failed download (attempt {attempt + 1}): Status {response.status_code}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return False\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url} (attempt {attempt + 1}): {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return False\n",
    "            time.sleep(1)\n",
    "    return False\n",
    "\n",
    "\n",
    "def prepare_galaxy_images(galaxies_df, output_dir='galaxy_images', image_size=64):\n",
    "    \"\"\"\n",
    "    Download and prepare SDSS images for a set of galaxies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    galaxies_df : pandas.DataFrame\n",
    "        DataFrame containing plate, mjd, fiber columns\n",
    "    output_dir : str\n",
    "        Directory to save images\n",
    "    image_size : int\n",
    "        Size of image cutouts in pixels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Original dataframe with additional columns for image paths and RA/DEC\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Lists to store results\n",
    "    image_paths = []\n",
    "    ras = []\n",
    "    decs = []\n",
    "    success = []\n",
    "    \n",
    "    def process_galaxy(row):\n",
    "        plate = int(row['plate'])\n",
    "        mjd = int(row['mjd'])\n",
    "        fiber = int(row['fiberid'])\n",
    "        \n",
    "        # Generate unique filename without any number suffixes\n",
    "        filename = f\"spec-{plate:04d}-{mjd}-{fiber:04d}.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Get image URL and coordinates\n",
    "        result = get_sdss_image_url(plate, mjd, fiber, \n",
    "                                  width=image_size, \n",
    "                                  height=image_size)\n",
    "        \n",
    "        if result is None:\n",
    "            return (output_path, None, None, False)\n",
    "            \n",
    "        url, ra, dec = result\n",
    "        \n",
    "        # Download image\n",
    "        success = download_image(url, output_path)\n",
    "        \n",
    "        return (output_path, ra, dec, success)\n",
    "    \n",
    "    # Process galaxies in parallel\n",
    "    print(\"Downloading galaxy images...\")\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(executor.map(process_galaxy, \n",
    "                                       [row for _, row in galaxies_df.iterrows()]), \n",
    "                          total=len(galaxies_df)))\n",
    "    \n",
    "    # Unpack results\n",
    "    image_paths, ras, decs, success = zip(*results)\n",
    "    \n",
    "    # Add columns to dataframe\n",
    "    galaxies_df['image_path'] = image_paths\n",
    "    galaxies_df['ra'] = ras\n",
    "    galaxies_df['dec'] = decs\n",
    "    galaxies_df['image_downloaded'] = success\n",
    "    \n",
    "    print(f\"\\nDownloaded {sum(success)} images successfully\")\n",
    "    print(f\"Failed to download {len(success) - sum(success)} images\")\n",
    "    \n",
    "    return galaxies_df"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:14.581360Z",
     "start_time": "2024-12-05T12:07:14.570121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class GalaxyClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(GalaxyClassifierCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs=10, device='cuda', patience=5):\n",
    "    \"\"\"\n",
    "    Train the CNN model\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def prepare_data_loaders(image_paths, labels, batch_size=32, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare train and validation data loaders\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Galaxies are orientation-invariant\n",
    "    transforms.RandomRotation(360),        # Full rotation\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.3,\n",
    "        contrast=0.3,\n",
    "        saturation=0.3,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        image_paths, labels, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = GalaxyDataset(X_train, y_train, transform=transform)\n",
    "    val_dataset = GalaxyDataset(X_val, y_val, transform=transform)\n",
    "    \n",
    "    # Create data loaders with num_workers=0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=0)  # Changed from 4 to 0\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=0)   # Changed from 4 to 0\n",
    "    \n",
    "    return train_loader, val_loader"
   ],
   "id": "b05f4aa2d56959a9",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:14.606319Z",
     "start_time": "2024-12-05T12:07:14.599513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/E-INSPIRE_I_master_catalogue.csv')  # Should have 'plate', 'mjd', 'fiberid' columns\n",
    "\n",
    "def create_dor_labels(dor_value):\n",
    "    if dor_value < 0.3:\n",
    "        return 0\n",
    "    elif dor_value < 0.6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Modify the label creation\n",
    "df['relic_label'] = df['DoR'].apply(lambda x: 0 if x < 0.4 else (1 if x < 0.5 else 2))\n",
    "\n",
    "print(df['relic_label'].value_counts())"
   ],
   "id": "d81f9e7d74a89bd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relic_label\n",
      "2    225\n",
      "0    104\n",
      "1    101\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:55.243853Z",
     "start_time": "2024-12-05T12:07:14.622629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before running prepare_galaxy_images\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clear existing downloaded images if any\n",
    "if os.path.exists('galaxy_images'):\n",
    "    shutil.rmtree('galaxy_images')\n",
    "os.makedirs('galaxy_images')\n",
    "\n",
    "print(f\"Attempting to download {len(df)} galaxy images...\")\n",
    "df_with_images = prepare_galaxy_images(df, output_dir='galaxy_images', image_size=64)\n",
    "\n",
    "print(\"\\nDownload Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total galaxies: {len(df)}\")\n",
    "print(f\"Successfully downloaded: {df_with_images['image_downloaded'].sum()}\")\n",
    "print(f\"Failed downloads: {len(df) - df_with_images['image_downloaded'].sum()}\")\n",
    "\n",
    "# Save successful downloads info\n",
    "successful = df_with_images[df_with_images['image_downloaded']]\n",
    "print(\"\\nFirst few successful downloads:\")\n",
    "print(successful[['plate', 'mjd', 'fiberid', 'image_path']].head())\n",
    "\n",
    "# After downloading images, let's check our class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df_with_images['relic_label'].value_counts())\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print(df_with_images['relic_label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Only use images that were successfully downloaded\n",
    "valid_data = df_with_images[df_with_images['image_downloaded']]\n",
    "print(f\"\\nNumber of usable images: {len(valid_data)}\")\n",
    "\n",
    "\n",
    "# After creating the labels\n",
    "print(\"Label distribution in full dataset:\")\n",
    "print(df['relic_label'].value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in valid data:\")\n",
    "print(valid_data['relic_label'].value_counts())\n",
    "\n",
    "\n",
    "if len(valid_data) == 0:\n",
    "    raise RuntimeError(\"No valid images found. Check the download process.\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = prepare_data_loaders(\n",
    "    valid_data['image_path'].values,\n",
    "    valid_data['relic_label'].values,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Initialize model and training parameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Add class weights to handle imbalance\n",
    "existing_classes = sorted(df['relic_label'].unique())\n",
    "class_counts = valid_data['relic_label'].value_counts()\n",
    "weights = torch.FloatTensor([\n",
    "    1/(class_counts[0] + 1e-5),\n",
    "    1/(class_counts[1] + 1e-5),\n",
    "    1/(class_counts[2] + 1e-5)\n",
    "])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "model = GalaxyClassifierCNN(num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reduced learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     num_epochs=20, device=device)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model and evaluate it\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(evaluate_model(model, val_loader, device))"
   ],
   "id": "4ea09e835c4fbdbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 430 galaxy images...\n",
      "Downloading galaxy images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 142/430 [00:11<00:17, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 167/430 [00:16<00:34,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 224/430 [00:20<00:17, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 256/430 [00:22<00:09, 17.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 298/430 [00:27<00:08, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 301/430 [00:28<00:22,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 2): Status 429\n",
      "Failed download (attempt 2): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 305/430 [00:30<00:30,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429Failed download (attempt 1): Status 429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 334/430 [00:30<00:06, 15.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 2): Status 429\n",
      "Failed download (attempt 2): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 344/430 [00:34<00:12,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 2): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 400/430 [00:38<00:02, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n",
      "Failed download (attempt 1): Status 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:40<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded 430 images successfully\n",
      "Failed to download 0 images\n",
      "\n",
      "Download Summary:\n",
      "--------------------------------------------------\n",
      "Total galaxies: 430\n",
      "Successfully downloaded: 430\n",
      "Failed downloads: 0\n",
      "\n",
      "First few successful downloads:\n",
      "   plate    mjd  fiberid                              image_path\n",
      "0   4405  55854      591  galaxy_images/spec-4405-55854-0591.jpg\n",
      "1   1342  52793      503  galaxy_images/spec-1342-52793-0503.jpg\n",
      "2   2775  54535      128  galaxy_images/spec-2775-54535-0128.jpg\n",
      "3    485  51909      432  galaxy_images/spec-0485-51909-0432.jpg\n",
      "4   1781  53297      341  galaxy_images/spec-1781-53297-0341.jpg\n",
      "\n",
      "Class Distribution:\n",
      "relic_label\n",
      "2    225\n",
      "0    104\n",
      "1    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage Distribution:\n",
      "relic_label\n",
      "2    52.325581\n",
      "0    24.186047\n",
      "1    23.488372\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of usable images: 430\n",
      "Label distribution in full dataset:\n",
      "relic_label\n",
      "2    225\n",
      "0    104\n",
      "1    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution in valid data:\n",
      "relic_label\n",
      "2    225\n",
      "0    104\n",
      "1    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/charl/Documents/PythonPlay/AS01_Testing/galaxy_images/spec-2759-54534-0136.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[128], line 75\u001B[0m\n\u001B[1;32m     72\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau(optimizer, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Plot training history\u001B[39;00m\n\u001B[1;32m     79\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n",
      "Cell \u001B[0;32mIn[126], line 88\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience)\u001B[0m\n\u001B[1;32m     85\u001B[0m train_correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     86\u001B[0m train_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 88\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_Testing/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_Testing/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_Testing/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_Testing/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[126], line 23\u001B[0m, in \u001B[0;36mGalaxyDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     22\u001B[0m     img_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[idx]\n\u001B[0;32m---> 23\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m     label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx]\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n",
      "File \u001B[0;32m~/Documents/PythonPlay/AS01_Testing/.venv/lib/python3.11/site-packages/PIL/Image.py:3469\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3466\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   3468\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3469\u001B[0m     fp \u001B[38;5;241m=\u001B[39m builtins\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3470\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/charl/Documents/PythonPlay/AS01_Testing/galaxy_images/spec-2759-54534-0136.jpg'"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:55.248581Z",
     "start_time": "2024-12-05T12:01:32.012410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Total galaxies in DataFrame:\", len(df))\n",
    "print(\"Total downloaded images:\", len(os.listdir('galaxy_images')) if os.path.exists('galaxy_images') else 0)"
   ],
   "id": "825686917d431917",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total galaxies in DataFrame: 430\n",
      "Total downloaded images: 612\n"
     ]
    }
   ],
   "execution_count": 116
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
